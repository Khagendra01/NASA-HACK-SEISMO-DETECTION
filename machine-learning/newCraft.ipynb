{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "cat_directory = './data/training/catalogs/'\n",
    "cat_file = cat_directory + 'S12_Grade_A.csv'\n",
    "cat = pd.read_csv(cat_file)\n",
    "\n",
    "row = cat.iloc[23]\n",
    "arrival_time = datetime.strptime(row['time_abs(%Y-%m-%dT%H:%M:%S.%f)'],'%Y-%m-%dT%H:%M:%S.%f')\n",
    "arrival_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "seismic_data = pd.read_csv('seismic_data.csv')\n",
    "event_data = arrival_time\n",
    "\n",
    "# Convert time_abs to datetime\n",
    "seismic_data['time_abs'] = pd.to_datetime(seismic_data['time_abs'])\n",
    "event_data['time_abs'] = pd.to_datetime(event_data['time_abs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data around event times\n",
    "def get_event_window(seismic_df, event_time, window_size):\n",
    "    start_time = event_time - pd.Timedelta(seconds=window_size)\n",
    "    end_time = event_time + pd.Timedelta(seconds=window_size)\n",
    "    return seismic_df[(seismic_df['time_abs'] >= start_time) & (seismic_df['time_abs'] <= end_time)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Define filter functions\n",
    "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
    "    nyq = 0.5 * fs  # Nyquist Frequency\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    return butter(order, [low, high], btype='band')\n",
    "\n",
    "def apply_filter(data, lowcut, highcut, fs):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Apply filter to seismic velocity data\n",
    "fs = 1 / seismic_data['time_rel'].diff().median()  # Sampling frequency\n",
    "seismic_data['velocity_filtered'] = apply_filter(seismic_data['velocity'], lowcut=0.1, highcut=10.0, fs=fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import detrend\n",
    "\n",
    "seismic_data['velocity_detrended'] = detrend(seismic_data['velocity_filtered'])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "seismic_data['velocity_normalized'] = scaler.fit_transform(seismic_data[['velocity_detrended']])\n",
    "\n",
    "\n",
    "\n",
    "seismic_data['velocity_normalized'].interpolate(method='linear', inplace=True)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "z_scores = np.abs((seismic_data['velocity_normalized'] - seismic_data['velocity_normalized'].mean()) / seismic_data['velocity_normalized'].std())\n",
    "seismic_data = seismic_data[z_scores < 3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_amplitude = seismic_data['velocity_normalized'].max()\n",
    "\n",
    "\n",
    "mean_velocity = seismic_data['velocity_normalized'].mean()\n",
    "std_velocity = seismic_data['velocity_normalized'].std()\n",
    "\n",
    "\n",
    "zero_crossings = ((seismic_data['velocity_normalized'][:-1] * seismic_data['velocity_normalized'][1:]) < 0).sum()\n",
    "\n",
    "\n",
    "autocorr = seismic_data['velocity_normalized'].autocorr(lag=1)\n",
    "\n",
    "\n",
    "fft_values = np.fft.fft(seismic_data['velocity_normalized'])\n",
    "fft_freq = np.fft.fftfreq(len(fft_values), d=1/fs)\n",
    "\n",
    "dominant_freq = fft_freq[np.argmax(np.abs(fft_values))]\n",
    "\n",
    "spectral_centroid = np.sum(fft_freq * np.abs(fft_values)) / np.sum(np.abs(fft_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_energy(fft_vals, freq_bins, low_freq, high_freq):\n",
    "    indices = np.where((freq_bins >= low_freq) & (freq_bins <= high_freq))\n",
    "    return np.sum(np.abs(fft_vals[indices]) ** 2)\n",
    "\n",
    "total_energy = np.sum(np.abs(fft_values) ** 2)\n",
    "low_band_energy = band_energy(fft_values, fft_freq, 0.1, 1.0)\n",
    "high_band_energy = band_energy(fft_values, fft_freq, 1.0, 10.0)\n",
    "band_energy_ratio = low_band_energy / total_energy\n",
    "\n",
    "\n",
    "import pywt\n",
    "\n",
    "scales = np.arange(1, 128)\n",
    "coefficients, frequencies = pywt.cwt(seismic_data['velocity_normalized'], scales, 'morl')\n",
    "\n",
    "\n",
    "\n",
    "wavelet_features = coefficients.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30  # seconds\n",
    "event_windows = []\n",
    "\n",
    "for _, event in event_data.iterrows():\n",
    "    event_time = event['time_abs']\n",
    "    window_data = get_event_window(seismic_data, event_time, window_size)\n",
    "    event_windows.append(window_data)\n",
    "\n",
    "\n",
    "seismic_data['label'] = 0  # Default to No Event\n",
    "\n",
    "for window in event_windows:\n",
    "    seismic_data.loc[window.index, 'label'] = 1\n",
    "\n",
    "class_counts = seismic_data['label'].value_counts()\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "X_resampled, y_resampled = rus.fit_resample(seismic_data.drop('label', axis=1), seismic_data['label'])\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(seismic_data.drop('label', axis=1), seismic_data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 128  # Number of samples per window\n",
    "step_size = 64       # Overlap between windows\n",
    "\n",
    "def create_windows(data, window_length, step_size):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    for i in range(0, len(data) - window_length, step_size):\n",
    "        window = data.iloc[i:i + window_length]\n",
    "        windows.append(window.drop('label', axis=1).values)\n",
    "        labels.append(window['label'].mode()[0])  # Majority label in the window\n",
    "    return np.array(windows), np.array(labels)\n",
    "\n",
    "X, y = create_windows(seismic_data, window_length, step_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    # Train and validate your model here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Wiener filter\n",
    "from scipy.signal import wiener\n",
    "\n",
    "seismic_data['velocity_adaptive_filtered'] = wiener(seismic_data['velocity_normalized'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train.reshape(len(X_train), -1), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(window_length, num_features)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "f, t, Sxx = spectrogram(seismic_data['velocity_normalized'], fs)\n",
    "plt.pcolormesh(t, f, Sxx, shading='gouraud')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(seismic_data['time_abs'], seismic_data['velocity_normalized'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Normalized Velocity')\n",
    "plt.title('Seismic Wiggle Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 0.01, seismic_data['velocity_normalized'].shape)\n",
    "seismic_data['velocity_augmented'] = seismic_data['velocity_normalized'] + noise\n",
    "\n",
    "seismic_data['velocity_shifted'] = seismic_data['velocity_normalized'].shift(1).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_velocity = seismic_data['velocity_normalized'].median()\n",
    "mad_velocity = seismic_data['velocity_normalized'].mad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.1)\n",
    "anomalies = iso_forest.fit_predict(seismic_data[['velocity_normalized']])\n",
    "seismic_data = seismic_data[anomalies == 1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
